{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP CA4 - Q1\n### Ali Khoramfar - 810102129","metadata":{}},{"cell_type":"markdown","source":"## Install Packages","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install --upgrade accelerate\n!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:55:10.991861Z","iopub.execute_input":"2024-05-27T14:55:10.992238Z","iopub.status.idle":"2024-05-27T14:55:50.731844Z","shell.execute_reply.started":"2024-05-27T14:55:10.992208Z","shell.execute_reply":"2024-05-27T14:55:50.730650Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nCollecting accelerate\n  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.29.3\n    Uninstalling accelerate-0.29.3:\n      Successfully uninstalled accelerate-0.29.3\nSuccessfully installed accelerate-0.30.1\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom accelerate import Accelerator\n\nfrom datasets import load_metric\nimport time\nimport logging\nimport evaluate\nimport torch\nfrom torchinfo import summary\n\nfrom peft import get_peft_model, LoraConfig,PromptTuningConfig, TaskType\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:55:50.734321Z","iopub.execute_input":"2024-05-27T14:55:50.734718Z","iopub.status.idle":"2024-05-27T14:56:09.634799Z","shell.execute_reply.started":"2024-05-27T14:55:50.734680Z","shell.execute_reply":"2024-05-27T14:56:09.634015Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-27 14:56:00.477843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 14:56:00.477939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 14:56:00.598295: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Multi NLI Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"multi_nli\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:56:09.635762Z","iopub.execute_input":"2024-05-27T14:56:09.636347Z","iopub.status.idle":"2024-05-27T14:56:23.127526Z","shell.execute_reply.started":"2024-05-27T14:56:09.636320Z","shell.execute_reply":"2024-05-27T14:56:23.126608Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f9948963d5471eaa282c1b51b93583"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 214M/214M [00:00<00:00, 251MB/s]  \nDownloading data: 100%|██████████| 4.94M/4.94M [00:00<00:00, 21.7MB/s]\nDownloading data: 100%|██████████| 5.10M/5.10M [00:00<00:00, 22.5MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09b89abd61304418acde477c464e3a84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92bab64b8a6d4569a5f3dc48bdbc54eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dec34c519594a9b9dfafb2db1b39b0e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Dataset Structure","metadata":{}},{"cell_type":"code","source":"print(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:56:23.129793Z","iopub.execute_input":"2024-05-27T14:56:23.130075Z","iopub.status.idle":"2024-05-27T14:56:23.135073Z","shell.execute_reply.started":"2024-05-27T14:56:23.130050Z","shell.execute_reply":"2024-05-27T14:56:23.134061Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n        num_rows: 392702\n    })\n    validation_matched: Dataset({\n        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n        num_rows: 9815\n    })\n    validation_mismatched: Dataset({\n        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n        num_rows: 9832\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# 10% of dataset\ntrain_subset = dataset['train'].shuffle(seed=42).select(range(int(0.1 * len(dataset['train']))))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:56:23.136507Z","iopub.execute_input":"2024-05-27T14:56:23.137169Z","iopub.status.idle":"2024-05-27T14:56:23.323452Z","shell.execute_reply.started":"2024-05-27T14:56:23.137118Z","shell.execute_reply":"2024-05-27T14:56:23.322330Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\nlogging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n\ndef preprocess_function(examples):\n    return tokenizer(\n        examples['premise'], \n        examples['hypothesis'], \n        truncation=True, \n        padding='max_length', \n        max_length=128\n    )\n\nencoded_train = train_subset.map(preprocess_function, batched=True)\nencoded_valid_matched = dataset['validation_matched'].map(preprocess_function, batched=True)\nencoded_valid_mismatched = dataset['validation_mismatched'].map(preprocess_function, batched=True)\n\nencoded_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nencoded_valid_matched.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nencoded_valid_mismatched.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:56:23.324870Z","iopub.execute_input":"2024-05-27T14:56:23.325204Z","iopub.status.idle":"2024-05-27T14:57:07.852033Z","shell.execute_reply.started":"2024-05-27T14:56:23.325178Z","shell.execute_reply":"2024-05-27T14:57:07.851143Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7faaddeb82491a9dacb9f07586f36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e661f1710d124dcd8b736ac9f4e7c13d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ece83603fc134c40b44763fc28f8526e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1dfef12bcf451ba3e325895085c379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3bf42f59cfc4b6a91dee77bdcfb3606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39270 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9448191db7fb47e79b2ad402213ed368"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b73f5f0e89b4a2a8d954d6d86cb9dc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b340d0a192c647b38c5da61db2acb6bb"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Load Roberta-Large Model","metadata":{}},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=3)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:57:07.853334Z","iopub.execute_input":"2024-05-27T14:57:07.853635Z","iopub.status.idle":"2024-05-27T14:57:13.935238Z","shell.execute_reply.started":"2024-05-27T14:57:07.853610Z","shell.execute_reply":"2024-05-27T14:57:13.934277Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b7b92c98f549759268ccd92daf533a"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training Arguments","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_steps=100000,\n    report_to=[],\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:57:13.936686Z","iopub.execute_input":"2024-05-27T14:57:13.937020Z","iopub.status.idle":"2024-05-27T14:57:13.989016Z","shell.execute_reply.started":"2024-05-27T14:57:13.936993Z","shell.execute_reply":"2024-05-27T14:57:13.988019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"metric = evaluate.load(\"accuracy\", trust_remote_code=True)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\naccelerator = Accelerator()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_valid_matched,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:57:13.990401Z","iopub.execute_input":"2024-05-27T14:57:13.990792Z","iopub.status.idle":"2024-05-27T14:57:15.257014Z","shell.execute_reply.started":"2024-05-27T14:57:13.990756Z","shell.execute_reply":"2024-05-27T14:57:15.256184Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b226cc19e74754aba9dea87a00af8b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train the Model  - roberta-large","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\ntrainer.train()\nend_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:47:01.292305Z","iopub.execute_input":"2024-05-26T11:47:01.292750Z","iopub.status.idle":"2024-05-26T13:19:51.414932Z","shell.execute_reply.started":"2024-05-26T11:47:01.292717Z","shell.execute_reply":"2024-05-26T13:19:51.414123Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7365' max='7365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7365/7365 1:32:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.510700</td>\n      <td>0.473953</td>\n      <td>0.840652</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.310800</td>\n      <td>0.449588</td>\n      <td>0.878859</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.218100</td>\n      <td>0.618880</td>\n      <td>0.877534</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Report","metadata":{}},{"cell_type":"code","source":"summary(model, input_size=(1, 128), dtypes=[torch.long])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:43:18.039654Z","iopub.execute_input":"2024-05-26T13:43:18.040291Z","iopub.status.idle":"2024-05-26T13:43:18.545033Z","shell.execute_reply.started":"2024-05-26T13:43:18.040253Z","shell.execute_reply":"2024-05-26T13:43:18.544107Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"==============================================================================================================\nLayer (type:depth-idx)                                       Output Shape              Param #\n==============================================================================================================\nRobertaForSequenceClassification                             [1, 3]                    --\n├─RobertaModel: 1-1                                          [1, 128, 1024]            --\n│    └─RobertaEmbeddings: 2-1                                [1, 128, 1024]            --\n│    │    └─Embedding: 3-1                                   [1, 128, 1024]            51,471,360\n│    │    └─Embedding: 3-2                                   [1, 128, 1024]            1,024\n│    │    └─Embedding: 3-3                                   [1, 128, 1024]            526,336\n│    │    └─LayerNorm: 3-4                                   [1, 128, 1024]            2,048\n│    │    └─Dropout: 3-5                                     [1, 128, 1024]            --\n│    └─RobertaEncoder: 2-2                                   [1, 128, 1024]            --\n│    │    └─ModuleList: 3-6                                  --                        302,309,376\n├─RobertaClassificationHead: 1-2                             [1, 3]                    --\n│    └─Dropout: 2-3                                          [1, 1024]                 --\n│    └─Linear: 2-4                                           [1, 1024]                 1,049,600\n│    └─Dropout: 2-5                                          [1, 1024]                 --\n│    └─Linear: 2-6                                           [1, 3]                    3,075\n==============================================================================================================\nTotal params: 355,362,819\nTrainable params: 355,362,819\nNon-trainable params: 0\nTotal mult-adds (M): 355.36\n==============================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 281.03\nParams size (MB): 1421.45\nEstimated Total Size (MB): 1702.48\n=============================================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"num_parameters = model.num_parameters()\nprint(f\"Number of Parameters: {num_parameters}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:21:33.838526Z","iopub.execute_input":"2024-05-26T13:21:33.839475Z","iopub.status.idle":"2024-05-26T13:21:33.846511Z","shell.execute_reply.started":"2024-05-26T13:21:33.839443Z","shell.execute_reply":"2024-05-26T13:21:33.845515Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Number of Parameters: 355362819\n","output_type":"stream"}]},{"cell_type":"code","source":"training_time = end_time - start_time\nprint(f\"Training time: {training_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:21:36.325291Z","iopub.execute_input":"2024-05-26T13:21:36.325667Z","iopub.status.idle":"2024-05-26T13:21:36.330972Z","shell.execute_reply.started":"2024-05-26T13:21:36.325639Z","shell.execute_reply":"2024-05-26T13:21:36.329932Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Training time: 5570.118634223938 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_results_matched = trainer.evaluate(encoded_valid_matched)\neval_results_mismatched = trainer.evaluate(encoded_valid_mismatched)\n\nprint(f\"Validation matched accuracy: {eval_results_matched['eval_accuracy']}\")\nprint(f\"Validation mismatched accuracy: {eval_results_mismatched['eval_accuracy']}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:21:39.647675Z","iopub.execute_input":"2024-05-26T13:21:39.648639Z","iopub.status.idle":"2024-05-26T13:26:09.513541Z","shell.execute_reply.started":"2024-05-26T13:21:39.648602Z","shell.execute_reply":"2024-05-26T13:26:09.512578Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1229' max='614' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [614/614 04:29]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation matched accuracy: 0.8775343861436576\nValidation mismatched accuracy: 0.8746948738812043\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/roberta-large-nli\")\ntokenizer.save_pretrained(\"/kaggle/working/roberta-large-nli\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:26:20.477821Z","iopub.execute_input":"2024-05-26T13:26:20.478654Z","iopub.status.idle":"2024-05-26T13:26:23.148330Z","shell.execute_reply.started":"2024-05-26T13:26:20.478624Z","shell.execute_reply":"2024-05-26T13:26:23.147364Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/roberta-large-nli/tokenizer_config.json',\n '/kaggle/working/roberta-large-nli/special_tokens_map.json',\n '/kaggle/working/roberta-large-nli/vocab.json',\n '/kaggle/working/roberta-large-nli/merges.txt',\n '/kaggle/working/roberta-large-nli/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:21:03.235677Z","iopub.execute_input":"2024-05-26T13:21:03.236576Z","iopub.status.idle":"2024-05-26T13:21:03.243378Z","shell.execute_reply.started":"2024-05-26T13:21:03.236517Z","shell.execute_reply":"2024-05-26T13:21:03.242463Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"transformers_version\": \"4.39.3\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"},"metadata":{}}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:37:33.464028Z","iopub.execute_input":"2024-05-26T13:37:33.464873Z","iopub.status.idle":"2024-05-26T13:37:33.472505Z","shell.execute_reply.started":"2024-05-26T13:37:33.464839Z","shell.execute_reply":"2024-05-26T13:37:33.471487Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train the Model  - LoRa","metadata":{}},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=3)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8,  \n    lora_alpha=32,  \n    lora_dropout=0.1  \n)\n\nmodel = get_peft_model(model, lora_config)\n\ntraining_args_lora = TrainingArguments(\n    output_dir=\"./results-lora\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs-lora',\n    logging_steps=10,\n    save_steps=100000,\n    report_to=[],  \n)\n\ntrainer_lora = Trainer(\n    model=model,\n    args=training_args_lora,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_valid_matched,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:52:54.861238Z","iopub.execute_input":"2024-05-26T13:52:54.861627Z","iopub.status.idle":"2024-05-26T13:52:56.285902Z","shell.execute_reply.started":"2024-05-26T13:52:54.861597Z","shell.execute_reply":"2024-05-26T13:52:56.284723Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time = time.time()\ntrainer_lora.train()\nend_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:53:13.483593Z","iopub.execute_input":"2024-05-26T13:53:13.483952Z","iopub.status.idle":"2024-05-26T15:00:57.971291Z","shell.execute_reply.started":"2024-05-26T13:53:13.483926Z","shell.execute_reply":"2024-05-26T15:00:57.970258Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7365' max='7365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7365/7365 1:07:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.511900</td>\n      <td>0.418702</td>\n      <td>0.846663</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.438200</td>\n      <td>0.384544</td>\n      <td>0.862150</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.471000</td>\n      <td>0.366670</td>\n      <td>0.867652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Report","metadata":{}},{"cell_type":"code","source":"summary(model, input_size=(1, 128), dtypes=[torch.long])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:13:58.689598Z","iopub.execute_input":"2024-05-26T15:13:58.690519Z","iopub.status.idle":"2024-05-26T15:13:59.779826Z","shell.execute_reply.started":"2024-05-26T15:13:58.690483Z","shell.execute_reply":"2024-05-26T15:13:59.778926Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"=============================================================================================================================\nLayer (type:depth-idx)                                                      Output Shape              Param #\n=============================================================================================================================\nPeftModelForSequenceClassification                                          [1, 3]                    --\n├─LoraModel: 1-1                                                            [1, 3]                    --\n│    └─RobertaForSequenceClassification: 2-1                                --                        --\n│    │    └─RobertaModel: 3-1                                               [1, 128, 1024]            355,096,576\n│    │    └─ModulesToSaveWrapper: 3-2                                       [1, 3]                    2,105,350\n=============================================================================================================================\nTotal params: 357,201,926\nTrainable params: 1,839,107\nNon-trainable params: 355,362,819\nTotal mult-adds (M): 356.15\n=============================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 331.75\nParams size (MB): 1424.60\nEstimated Total Size (MB): 1756.35\n============================================================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"num_parameters = model.num_parameters()\nprint(f\"Number of Parameters (LoRA): {num_parameters}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:14:08.369157Z","iopub.execute_input":"2024-05-26T15:14:08.369739Z","iopub.status.idle":"2024-05-26T15:14:08.378595Z","shell.execute_reply.started":"2024-05-26T15:14:08.369708Z","shell.execute_reply":"2024-05-26T15:14:08.377663Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Number of Parameters (LoRA): 357201926\n","output_type":"stream"}]},{"cell_type":"code","source":"training_time = end_time - start_time\nprint(f\"Training time (LoRA): {training_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:14:11.979050Z","iopub.execute_input":"2024-05-26T15:14:11.979719Z","iopub.status.idle":"2024-05-26T15:14:11.984070Z","shell.execute_reply.started":"2024-05-26T15:14:11.979683Z","shell.execute_reply":"2024-05-26T15:14:11.983091Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Training time (LoRA): 4064.483451604843 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_results_matched = trainer_lora.evaluate(encoded_valid_matched)\neval_results_mismatched = trainer_lora.evaluate(encoded_valid_mismatched)\n\nprint(f\"Validation matched accuracy (LoRA): {eval_results_matched['eval_accuracy']}\")\nprint(f\"Validation mismatched accuracy (LoRA): {eval_results_mismatched['eval_accuracy']}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:14:17.413688Z","iopub.execute_input":"2024-05-26T15:14:17.414038Z","iopub.status.idle":"2024-05-26T15:18:54.373281Z","shell.execute_reply.started":"2024-05-26T15:14:17.414010Z","shell.execute_reply":"2024-05-26T15:18:54.372329Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1229' max='614' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [614/614 04:36]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation matched accuracy (LoRA): 0.8676515537442689\nValidation mismatched accuracy (LoRA): 0.8668633034987795\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nmodel.save_pretrained(\"/kaggle/working/roberta-large-nli-lora\")\ntokenizer.save_pretrained(\"/kaggle/working/roberta-large-nli-lora\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:19:18.715102Z","iopub.execute_input":"2024-05-26T15:19:18.715463Z","iopub.status.idle":"2024-05-26T15:19:19.608041Z","shell.execute_reply.started":"2024-05-26T15:19:18.715435Z","shell.execute_reply":"2024-05-26T15:19:19.607016Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/roberta-large-nli-lora/tokenizer_config.json',\n '/kaggle/working/roberta-large-nli-lora/special_tokens_map.json',\n '/kaggle/working/roberta-large-nli-lora/vocab.json',\n '/kaggle/working/roberta-large-nli-lora/merges.txt',\n '/kaggle/working/roberta-large-nli-lora/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:19:23.434946Z","iopub.execute_input":"2024-05-26T15:19:23.435599Z","iopub.status.idle":"2024-05-26T15:19:23.442768Z","shell.execute_reply.started":"2024-05-26T15:19:23.435567Z","shell.execute_reply":"2024-05-26T15:19:23.441867Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"transformers_version\": \"4.39.3\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"},"metadata":{}}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:20:28.089792Z","iopub.execute_input":"2024-05-26T15:20:28.090204Z","iopub.status.idle":"2024-05-26T15:20:28.102460Z","shell.execute_reply.started":"2024-05-26T15:20:28.090171Z","shell.execute_reply":"2024-05-26T15:20:28.101590Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 1024)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-23): 24 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): ModulesToSaveWrapper(\n        (original_module): RobertaClassificationHead(\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n        )\n        (modules_to_save): ModuleDict(\n          (default): RobertaClassificationHead(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n          )\n        )\n      )\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train the Model  - Prompt Tuning","metadata":{}},{"cell_type":"code","source":"\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=3)\n\nprompt_tuning_config = PromptTuningConfig(\n    task_type=TaskType.SEQ_CLS,\n    num_virtual_tokens=30,  \n)\n\nmodel = get_peft_model(model, prompt_tuning_config)\n\nfor param in model.parameters():\n    param.requires_grad = False\nfor param in model.get_input_embeddings().parameters():\n    param.requires_grad = True\n    \n    \ntraining_args_prompt = TrainingArguments(\n    output_dir=\"./results-prompt-tuning\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=4e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs-prompt-tuning',\n    logging_steps=10,\n    save_steps=100000,\n    report_to=[],  \n)\n\ntrainer_prompt = Trainer(\n    model=model,\n    args=training_args_prompt,\n    train_dataset=encoded_train,\n    eval_dataset=encoded_valid_matched,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:57:28.190508Z","iopub.execute_input":"2024-05-27T14:57:28.190889Z","iopub.status.idle":"2024-05-27T14:57:29.611289Z","shell.execute_reply.started":"2024-05-27T14:57:28.190860Z","shell.execute_reply":"2024-05-27T14:57:29.610464Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time = time.time()\ntrainer_prompt.train()\nend_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T14:57:30.528031Z","iopub.execute_input":"2024-05-27T14:57:30.528448Z","iopub.status.idle":"2024-05-27T16:11:57.395880Z","shell.execute_reply.started":"2024-05-27T14:57:30.528419Z","shell.execute_reply":"2024-05-27T16:11:57.394926Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7365' max='7365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7365/7365 1:14:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.193300</td>\n      <td>1.126348</td>\n      <td>0.354457</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.173000</td>\n      <td>1.124661</td>\n      <td>0.354457</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.134500</td>\n      <td>1.124166</td>\n      <td>0.354457</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Report","metadata":{}},{"cell_type":"code","source":"summary(model, input_size=(1, 128), dtypes=[torch.long])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:15:17.640850Z","iopub.execute_input":"2024-05-27T16:15:17.641524Z","iopub.status.idle":"2024-05-27T16:15:17.896332Z","shell.execute_reply.started":"2024-05-27T16:15:17.641479Z","shell.execute_reply":"2024-05-27T16:15:17.895387Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"===================================================================================================================\nLayer (type:depth-idx)                                            Output Shape              Param #\n===================================================================================================================\nPeftModelForSequenceClassification                                [1, 3]                    --\n├─Embedding: 1-1                                                  [1, 128, 1024]            51,471,360\n├─ModuleDict: 1-2                                                 --                        --\n│    └─PromptEmbedding: 2-1                                       [1, 30, 1024]             --\n│    │    └─Embedding: 3-1                                        [1, 30, 1024]             (30,720)\n├─RobertaForSequenceClassification: 1-3                           [1, 3]                    --\n│    └─RobertaModel: 2-2                                          [1, 158, 1024]            --\n│    │    └─RobertaEmbeddings: 3-2                                [1, 158, 1024]            52,000,768\n│    │    └─RobertaEncoder: 3-3                                   [1, 158, 1024]            (302,309,376)\n│    └─ModulesToSaveWrapper: 2-3                                  [1, 3]                    1,052,675\n│    │    └─ModuleDict: 3-4                                       --                        (1,052,675)\n===================================================================================================================\nTotal params: 407,917,574\nTrainable params: 102,942,720\nNon-trainable params: 304,974,854\nTotal mult-adds (M): 355.39\n===================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 346.89\nParams size (MB): 1421.57\nEstimated Total Size (MB): 1768.47\n==================================================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"num_parameters = model.num_parameters()\nprint(f\"Number of Parameters (Prompt Tuning): {num_parameters}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:15:18.857055Z","iopub.execute_input":"2024-05-27T16:15:18.857925Z","iopub.status.idle":"2024-05-27T16:15:18.865251Z","shell.execute_reply.started":"2024-05-27T16:15:18.857890Z","shell.execute_reply":"2024-05-27T16:15:18.864037Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Number of Parameters (Prompt Tuning): 356415494\n","output_type":"stream"}]},{"cell_type":"code","source":"training_time = end_time - start_time\nprint(f\"Training time (Prompt Tuning): {training_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:15:19.371606Z","iopub.execute_input":"2024-05-27T16:15:19.371952Z","iopub.status.idle":"2024-05-27T16:15:19.377282Z","shell.execute_reply.started":"2024-05-27T16:15:19.371926Z","shell.execute_reply":"2024-05-27T16:15:19.376129Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training time (Prompt Tuning): 4466.86380147934 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_results_matched = trainer_prompt.evaluate(encoded_valid_matched)\neval_results_mismatched = trainer_prompt.evaluate(encoded_valid_mismatched)\n\nprint(f\"Validation matched accuracy (Prompt Tuning): {eval_results_matched['eval_accuracy']}\")\nprint(f\"Validation mismatched accuracy (Prompt Tuning): {eval_results_mismatched['eval_accuracy']}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:15:19.897869Z","iopub.execute_input":"2024-05-27T16:15:19.898749Z","iopub.status.idle":"2024-05-27T16:20:40.660886Z","shell.execute_reply.started":"2024-05-27T16:15:19.898706Z","shell.execute_reply":"2024-05-27T16:20:40.659773Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1229' max='614' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [614/614 05:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation matched accuracy (Prompt Tuning): 0.3544574630667346\nValidation mismatched accuracy (Prompt Tuning): 0.3522172497965826\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/roberta-large-nli-prompt-tuning\")\ntokenizer.save_pretrained(\"/kaggle/working/roberta-large-nli-prompt-tuning\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:20:40.663050Z","iopub.execute_input":"2024-05-27T16:20:40.663471Z","iopub.status.idle":"2024-05-27T16:20:41.171377Z","shell.execute_reply.started":"2024-05-27T16:20:40.663434Z","shell.execute_reply":"2024-05-27T16:20:41.170438Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/roberta-large-nli-prompt-tuning/tokenizer_config.json',\n '/kaggle/working/roberta-large-nli-prompt-tuning/special_tokens_map.json',\n '/kaggle/working/roberta-large-nli-prompt-tuning/vocab.json',\n '/kaggle/working/roberta-large-nli-prompt-tuning/merges.txt',\n '/kaggle/working/roberta-large-nli-prompt-tuning/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:20:48.735245Z","iopub.execute_input":"2024-05-27T16:20:48.736133Z","iopub.status.idle":"2024-05-27T16:20:48.743072Z","shell.execute_reply.started":"2024-05-27T16:20:48.736086Z","shell.execute_reply":"2024-05-27T16:20:48.742115Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"RobertaConfig {\n  \"_name_or_path\": \"roberta-large\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"transformers_version\": \"4.39.3\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"},"metadata":{}}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T16:20:49.763376Z","iopub.execute_input":"2024-05-27T16:20:49.764049Z","iopub.status.idle":"2024-05-27T16:20:49.771904Z","shell.execute_reply.started":"2024-05-27T16:20:49.764020Z","shell.execute_reply":"2024-05-27T16:20:49.770866Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"PeftModelForSequenceClassification(\n  (base_model): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 1024)\n        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): RobertaEncoder(\n        (layer): ModuleList(\n          (0-23): 24 x RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=1024, out_features=1024, bias=True)\n                (key): Linear(in_features=1024, out_features=1024, bias=True)\n                (value): Linear(in_features=1024, out_features=1024, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (classifier): ModulesToSaveWrapper(\n      (original_module): RobertaClassificationHead(\n        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n      )\n      (modules_to_save): ModuleDict(\n        (default): RobertaClassificationHead(\n          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n        )\n      )\n    )\n  )\n  (prompt_encoder): ModuleDict(\n    (default): PromptEmbedding(\n      (embedding): Embedding(30, 1024)\n    )\n  )\n  (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}